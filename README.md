# Learning Affordance Grounding from Exocentric Images
PyTorch implementation of our one-shot affordance detection models. This repository contains PyTorch evaluation code, training code.
1. [ğŸ“ Paper Link](#1)
2. [ğŸ’¡ Abstract](#2)
3. [ğŸ“– Method](#3)
4. [ğŸ“‚ Dataset](#4)
5. [ğŸ“ƒ Requirements](#5)
6. [ğŸ“Š Experimental Results](#6)
7. [âœ‰ï¸ Statement](#7)
8. [ğŸ” Citation](#8)

## ğŸ“ Paper Link <a name="1"></a> 
* Learning Affordance Grounding from Exocentric Images (CVPR2022) ([link]())
> Authors:
> Hongchen Luo, Wei Zhai, Jing Zhang, Yang Cao, Dacheng Tao

## ğŸ’¡ Abstract <a name="2"></a> 
Affordance grounding, a task to ground (i.e., localize) action possibility region in objects, which faces the challenge of establishing an explicit link with object parts due to the diversity of interactive affordance. Human has the ability that transform the various exocentric interactions to invariant egocentric affordance so as to counter the impact of interactive diversity. To empower an agent with such an ability, this paper proposes a task of affordance grounding from exocentric view, i.e., given exocentric human-object interaction and egocentric object images, learning the affordance knowledge of the object and transferring it to the egocentric image using only the affordance label as supervision. To this end, we devise a cross-view knowledge transfer framework that extracts affordance-specific features from exocentric interactions and enhances the perception of affordance regions by preserving affordance correlation. Specifically, an Affordance Invariance Mining module is devised to extract specific clues by minimizing the intra-class differences originated from interaction habits in exocentric images. Furthermore, an Affordnace Co-relation Preserving strategy is presented to perceive and localize affordance by aligning the co-relation matrix of predicted results between the two views. Particularly, an affordance grounding dataset named AGD20K is constructed by collecting and labeling over 20K images from 36 affordance categories. Experimental results demonstrate that our method outperforms the representative methods in terms of objective metrics and visual quality.


